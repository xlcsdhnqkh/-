# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jeDAzMQ3ynR8YFchuu863TmuZ3yaRI1A
"""

!pip install datasets

from datasets import load_dataset

dataset = load_dataset("imdb")

print(dataset)

print(dataset["train"][0])

!pip install transformers

from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

text = "This movie is absolute trash"

tokens = tokenizer(text)

print(tokens)

import torch
import torch.nn as nn

vocab_size = tokenizer.vocab_size
embedding_dim = 128

embedding = nn.Embedding(vocab_size, embedding_dim)

input_ids = torch.tensor(tokens["input_ids"])

embedded = embedding(input_ids)

print(embedded.shape)

d_model = 128
W_q = nn.Linear(d_model, d_model)
W_k = nn.Linear(d_model, d_model)
W_v = nn.Linear(d_model, d_model)

Q = W_q(embedded)
K = W_k(embedded)
V = W_v(embedded)

print(Q.shape)

import math

scores = torch.matmul(Q, K.transpose(0,1)) / math.sqrt(d_model)

print(scores.shape)

attention_weights = torch.softmax(scores, dim=-1)

print(attention_weights.shape)

output = torch.matmul(attention_weights, V)

print(output.shape)

cls_vector = output[0]   # 取第一个token
print(cls_vector.shape)

classifier = nn.Linear(128, 2)

logits = classifier(cls_vector)

print(logits)

prediction = torch.argmax(logits)

print(prediction)

import torch
from torch.utils.data import DataLoader
from torch.nn.utils.rnn import pad_sequence

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 只取一部分数据加快训练
train_data = dataset["train"].select(range(2000))
test_data = dataset["test"].select(range(1000))

def tokenize(example):
    return tokenizer(
        example["text"],
        truncation=True,
        padding="max_length",
        max_length=128
    )

train_data = train_data.map(tokenize, batched=True)
test_data = test_data.map(tokenize, batched=True)

train_data.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])
test_data.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])

train_loader = DataLoader(train_data, batch_size=32, shuffle=True)
test_loader = DataLoader(test_data, batch_size=32)

import torch.nn as nn

class SimpleTransformer(nn.Module):
    def __init__(self, vocab_size, d_model=128, nhead=4):
        super().__init__()

        self.embedding = nn.Embedding(vocab_size, d_model)

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            batch_first=True
        )

        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)

        self.fc = nn.Linear(d_model, 2)

    def forward(self, input_ids):
        x = self.embedding(input_ids)
        x = self.transformer(x)

        cls_token = x[:, 0, :]   # 取CLS
        out = self.fc(cls_token)

        return out

model = SimpleTransformer(tokenizer.vocab_size).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

for epoch in range(2):
    model.train()
    total_loss = 0

    for batch in train_loader:
        input_ids = batch["input_ids"].to(device)
        labels = batch["label"].to(device)

        optimizer.zero_grad()

        outputs = model(input_ids)
        loss = criterion(outputs, labels)

        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch {epoch+1}, Loss: {total_loss/len(train_loader)}")

def predict(text):
    model.eval()

    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding="max_length", max_length=128)
    input_ids = inputs["input_ids"].to(device)

    with torch.no_grad():
        outputs = model(input_ids)
        pred = torch.argmax(outputs, dim=1).item()

    return "Positive" if pred == 1 else "Negative"

print(predict("This movie is absolute trash"))
print(predict("Masterpiece"))

model.eval()
correct = 0
total = 0

with torch.no_grad():
    for batch in test_loader:
        input_ids = batch["input_ids"].to(device)
        labels = batch["label"].to(device)

        outputs = model(input_ids)
        preds = torch.argmax(outputs, dim=1)

        correct += (preds == labels).sum().item()
        total += labels.size(0)

print("Test Accuracy:", correct / total)

print(id(train_data))
print(id(test_data))

print(train_data[0]["input_ids"][:10])
print(test_data[0]["input_ids"][:10])

# 查看测试集标签分布
labels = [example["label"] for example in test_data]
print("Positive count:", sum(labels))
print("Total:", len(labels))

import random

indices = random.sample(range(len(dataset["test"])), 1000)
test_data = dataset["test"].select(indices)

test_data = test_data.map(tokenize, batched=True)
test_data.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])

test_loader = DataLoader(test_data, batch_size=32)

model.eval()
correct = 0
total = 0

with torch.no_grad():
    for batch in test_loader:
        input_ids = batch["input_ids"].to(device)
        labels = batch["label"].to(device)

        outputs = model(input_ids)
        preds = torch.argmax(outputs, dim=1)

        correct += (preds == labels).sum().item()
        total += labels.size(0)

print("Test Accuracy:", correct / total)

from transformers import pipeline

classifier = pipeline(
    "sentiment-analysis",
    model="textattack/bert-base-uncased-imdb"
)

print(classifier("This movie is absolute trash"))
print(classifier("Masterpiece"))

def predict(text):
    result = classifier(text)[0]
    label = result["label"]

    if label == "LABEL_0":
        sentiment = "Negative"
    else:
        sentiment = "Positive"

    print(f"Text: {text}")
    print(f"Sentiment: {sentiment}")
    print(f"Confidence: {result['score']:.4f}")
    print("-" * 40)

predict("This movie is absolute trash")
predict("Masterpiece")